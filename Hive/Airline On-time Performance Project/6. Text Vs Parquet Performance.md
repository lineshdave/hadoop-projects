# Text Vs Parquet Performance

## Query Performance
<pre>
-- external text table query
select count(1) from airline_timing;

INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:7
INFO  : Submitting tokens for job: job_1494894036184_3237
INFO  : The url to track the job: http://iop-bi-master.imdemocloud.com:8088/proxy/application_1494894036184_3237/
INFO  : Starting Job = job_1494894036184_3237, Tracking URL = http://iop-bi-master.imdemocloud.com:8088/proxy/application_1494894036184_3237/
INFO  : Kill Command = /usr/iop/4.1.0.0/hadoop/bin/hadoop job  -kill job_1494894036184_3237
INFO  : Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 1
INFO  : 2017-07-08 21:48:42,065 Stage-1 map = 0%,  reduce = 0%
INFO  : 2017-07-08 21:48:49,244 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 38.36 sec
INFO  : 2017-07-08 21:48:55,385 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 41.07 sec
INFO  : MapReduce Total cumulative CPU time: 41 seconds 70 msec
INFO  : Ended Job = job_1494894036184_3237
+-----------+--+
|    _c0    |
+-----------+--+
| 21604865  |
+-----------+--+
1 row selected (19.556 seconds)

-- second query
select year, count(1) from airline_timing group by year;

INFO  : Number of reduce tasks not specified. Estimated from input data size: 8
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:7
INFO  : Submitting tokens for job: job_1494894036184_3296
INFO  : The url to track the job: http://iop-bi-master.imdemocloud.com:8088/proxy/application_1494894036184_3296/
INFO  : Starting Job = job_1494894036184_3296, Tracking URL = http://iop-bi-master.imdemocloud.com:8088/proxy/application_1494894036184_3296/
INFO  : Kill Command = /usr/iop/4.1.0.0/hadoop/bin/hadoop job  -kill job_1494894036184_3296
INFO  : Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 8
INFO  : 2017-07-10 08:17:50,796 Stage-1 map = 0%,  reduce = 0%
INFO  : 2017-07-10 08:17:58,991 Stage-1 map = 71%,  reduce = 0%, Cumulative CPU 37.2 sec
INFO  : 2017-07-10 08:18:00,013 Stage-1 map = 86%,  reduce = 0%, Cumulative CPU 44.93 sec
INFO  : 2017-07-10 08:18:01,041 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 53.68 sec
INFO  : 2017-07-10 08:18:04,108 Stage-1 map = 100%,  reduce = 38%, Cumulative CPU 60.29 sec
INFO  : 2017-07-10 08:18:05,131 Stage-1 map = 100%,  reduce = 75%, Cumulative CPU 66.05 sec
INFO  : 2017-07-10 08:18:06,159 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 70.92 sec
INFO  : MapReduce Total cumulative CPU time: 1 minutes 10 seconds 920 msec
INFO  : Ended Job = job_1494894036184_3296
+-------+----------+--+
| year  |   _c1    |
+-------+----------+--+
| 2008  | 7009728  |
| 2006  | 7141922  |
| 2007  | 7453215  |
+-------+----------+--+
3 rows selected (21.571 seconds)


-- Move over to parquet based table

-- external parquet table query
select count(1) from pq_airline_timing;

+-----------+--+
|    _c0    |
+-----------+--+
| 21604865  |
+-----------+--+
1 row selected (0.044 seconds)

-- second query
select year, count(1) from pq_airline_timing group by year;

INFO  : Number of reduce tasks not specified. Estimated from input data size: 2
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:2
INFO  : Submitting tokens for job: job_1494894036184_3297
INFO  : The url to track the job: http://iop-bi-master.imdemocloud.com:8088/proxy/application_1494894036184_3297/
INFO  : Starting Job = job_1494894036184_3297, Tracking URL = http://iop-bi-master.imdemocloud.com:8088/proxy/application_1494894036184_3297/
INFO  : Kill Command = /usr/iop/4.1.0.0/hadoop/bin/hadoop job  -kill job_1494894036184_3297
INFO  : Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 2
INFO  : 2017-07-10 10:40:57,083 Stage-1 map = 0%,  reduce = 0%
INFO  : 2017-07-10 10:41:07,307 Stage-1 map = 69%,  reduce = 0%, Cumulative CPU 25.07 sec
INFO  : 2017-07-10 10:41:09,351 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 27.93 sec
INFO  : 2017-07-10 10:41:12,420 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 32.72 sec
INFO  : MapReduce Total cumulative CPU time: 32 seconds 720 msec
INFO  : Ended Job = job_1494894036184_3297
+-------+----------+--+
| year  |   _c1    |
+-------+----------+--+
| 2006  | 7141922  |
| 2008  | 7009728  |
| 2007  | 7453215  |
+-------+----------+--+
3 rows selected (21.512 seconds)
</pre>

## Memory Consumption
<pre>
hdfs dfs -ls -R -h /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/

-rw-rw----+  3 ldave2001 ldave2001    128.5 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00000
-rw-rw----+  3 ldave2001 ldave2001    127.7 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00001
-rw-rw----+  3 ldave2001 ldave2001    127.8 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00002
-rw-rw----+  3 ldave2001 ldave2001    127.7 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00003
-rw-rw----+  3 ldave2001 ldave2001    127.7 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00004
-rw-rw----+  3 ldave2001 ldave2001    127.4 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00005
-rw-rw----+  3 ldave2001 ldave2001    127.7 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00006
-rw-rw----+  3 ldave2001 ldave2001    127.6 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00007
-rw-rw----+  3 ldave2001 ldave2001    127.7 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00008
-rw-rw----+  3 ldave2001 ldave2001    127.7 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00009
-rw-rw----+  3 ldave2001 ldave2001    117.6 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00010
-rw-rw----+  3 ldave2001 ldave2001    117.4 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00011
-rw-rw----+  3 ldave2001 ldave2001    117.7 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00012
-rw-rw----+  3 ldave2001 ldave2001    116.8 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00013
-rw-rw----+  3 ldave2001 ldave2001    117.3 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00014
-rw-rw----+  3 ldave2001 ldave2001     46.4 M 2017-07-08 19:17 /user/ldave2001/rawdata/handson_train/airline_performance/flights_processed/part-m-00015

hdfs dfs -ls -R -h /user/ldave2001/rawdata/handson_train/airline_performance/flights_parquet/

-rwxrwx--x+  3 ldave2001 ldave2001     77.7 M 2017-07-08 19:27 /user/ldave2001/rawdata/handson_train/airline_performance/flights_parquet/000000_0
-rwxrwx--x+  3 ldave2001 ldave2001     60.3 M 2017-07-08 19:26 /user/ldave2001/rawdata/handson_train/airline_performance/flights_parquet/000001_0
-rwxrwx--x+  3 ldave2001 ldave2001     54.9 M 2017-07-08 19:26 /user/ldave2001/rawdata/handson_train/airline_performance/flights_parquet/000002_0
-rwxrwx--x+  3 ldave2001 ldave2001     54.9 M 2017-07-08 19:26 /user/ldave2001/rawdata/handson_train/airline_performance/flights_parquet/000003_0
-rwxrwx--x+  3 ldave2001 ldave2001     54.0 M 2017-07-08 19:26 /user/ldave2001/rawdata/handson_train/airline_performance/flights_parquet/000004_0
-rwxrwx--x+  3 ldave2001 ldave2001     54.6 M 2017-07-08 19:26 /user/ldave2001/rawdata/handson_train/airline_performance/flights_parquet/000005_0
-rwxrwx--x+  3 ldave2001 ldave2001     55.5 M 2017-07-08 19:26 /user/ldave2001/rawdata/handson_train/airline_performance/flights_parquet/000006_0
</pre>

## Conclusion
It is evident from the query output and display due to "hdfs ls" command that the parquet data format is far more efficient both from storage and retrieval perspective. Even the actual data values that were retrieved due to the queries were exactly the same. So, we can conclude from this exercise that parquet format is a better choice than text for big data.
